"""
Nifty Live Stock Market Prediction Model for VS Code
====================================================
A production-ready deep learning model for Nifty 50 predictions with live data integration.

Requirements:
pip install tensorflow yfinance pandas numpy matplotlib seaborn scikit-learn ta requests beautifulsoup4

Author: AI Assistant
Date: 2025
"""

import os
import sys
import warnings
warnings.filterwarnings('ignore')

# Set environment variables for better performance
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import yfinance as yf
from datetime import datetime, timedelta
import json
import pickle
from pathlib import Path

# Try importing tensorflow with fallback
try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, Model, load_model
    from tensorflow.keras.layers import LSTM, Dense, Dropout, Conv1D, MaxPooling1D, Input
    from tensorflow.keras.optimizers import Adam
    from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
    print("‚úÖ TensorFlow loaded successfully")
except ImportError as e:
    print(f"‚ùå Error importing TensorFlow: {e}")
    print("Please install tensorflow: pip install tensorflow")
    sys.exit(1)

# Try importing technical analysis library
try:
    import ta
    print("‚úÖ Technical Analysis library loaded")
except ImportError:
    print("‚ö†Ô∏è Installing technical analysis library...")
    os.system("pip install ta")
    import ta

class LiveNiftyPredictor:
    """
    A comprehensive live stock market prediction model for Nifty 50
    """
    
    def __init__(self, lookback_period=60, prediction_horizon=5):
        self.lookback_period = lookback_period
        self.prediction_horizon = prediction_horizon
        self.scaler = MinMaxScaler()
        self.feature_scaler = StandardScaler()
        self.model = None
        self.history = None
        self.feature_columns = None
        
        # Create directories for saving models and data
        self.model_dir = Path("models")
        self.data_dir = Path("data")
        self.model_dir.mkdir(exist_ok=True)
        self.data_dir.mkdir(exist_ok=True)
        
        print("üöÄ LiveNiftyPredictor initialized successfully!")
    
    def fetch_live_nifty_data(self, period="2y", interval="1d"):
        """
        Fetch live Nifty 50 data with error handling and fallback options
        """
        print("üì° Fetching live Nifty data...")
        
        # Multiple symbols to try
        nifty_symbols = ["^NSEI", "NIFTY50.NS", "^NSEBANK"]
        
        for symbol in nifty_symbols:
            try:
                ticker = yf.Ticker(symbol)
                data = ticker.history(period=period, interval=interval)
                
                if len(data) > 100:  # Ensure we have enough data
                    print(f"‚úÖ Successfully fetched {len(data)} days of data from {symbol}")
                    
                    # Add symbol info
                    data['Symbol'] = symbol
                    
                    # Clean data
                    data = data.dropna()
                    
                    # Save raw data
                    data.to_csv(self.data_dir / f"raw_nifty_data_{datetime.now().strftime('%Y%m%d')}.csv")
                    
                    return data
                    
            except Exception as e:
                print(f"‚ö†Ô∏è Failed to fetch data from {symbol}: {e}")
                continue
        
        print("‚ùå All data sources failed. Using backup sample data.")
        return self.generate_realistic_sample_data()
    
    def generate_realistic_sample_data(self, days=1500):
        """
        Generate realistic sample data based on actual market patterns
        """
        print("üé≤ Generating realistic sample data...")
        
        dates = pd.date_range(start='2020-01-01', periods=days, freq='D')
        np.random.seed(42)
        
        # Start with a realistic Nifty base price
        base_price = 12000
        prices = []
        volumes = []
        
        # Market trends and cycles
        for i in range(days):
            # Add market cycles and trends
            trend = 0.0002 * np.sin(i * 2 * np.pi / 252)  # Annual cycle
            volatility = 0.015 + 0.01 * np.sin(i * 2 * np.pi / 60)  # Volatility cycles
            
            # Random walk with trend
            change = np.random.normal(trend, volatility)
            base_price *= (1 + change)
            
            # Ensure price doesn't go negative
            base_price = max(base_price, 1000)
            
            # Calculate OHLC from close price
            close = base_price
            open_price = close * np.random.uniform(0.995, 1.005)
            high = max(open_price, close) * np.random.uniform(1.0, 1.02)
            low = min(open_price, close) * np.random.uniform(0.98, 1.0)
            
            prices.append([open_price, high, low, close])
            volumes.append(np.random.randint(500000, 3000000))
        
        # Create DataFrame
        price_data = np.array(prices)
        data = pd.DataFrame({
            'Open': price_data[:, 0],
            'High': price_data[:, 1],
            'Low': price_data[:, 2],
            'Close': price_data[:, 3],
            'Volume': volumes
        }, index=dates)
        
        return data
    
    def calculate_advanced_indicators(self, data):
        """
        Calculate comprehensive technical indicators using the TA library
        """
        print("üîß Calculating advanced technical indicators...")
        
        df = data.copy()
        
        # Ensure we have the required columns
        required_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
        for col in required_cols:
            if col not in df.columns:
                print(f"‚ùå Missing column: {col}")
                return None
        
        try:
            # Trend Indicators
            df['SMA_5'] = ta.trend.sma_indicator(df['Close'], window=5)
            df['SMA_10'] = ta.trend.sma_indicator(df['Close'], window=10)
            df['SMA_20'] = ta.trend.sma_indicator(df['Close'], window=20)
            df['SMA_50'] = ta.trend.sma_indicator(df['Close'], window=50)
            df['EMA_12'] = ta.trend.ema_indicator(df['Close'], window=12)
            df['EMA_26'] = ta.trend.ema_indicator(df['Close'], window=26)
            
            # MACD
            df['MACD'] = ta.trend.macd(df['Close'])
            df['MACD_signal'] = ta.trend.macd_signal(df['Close'])
            df['MACD_diff'] = ta.trend.macd_diff(df['Close'])
            
            # Bollinger Bands
            bollinger = ta.volatility.BollingerBands(df['Close'])
            df['BB_high'] = bollinger.bollinger_hband()
            df['BB_low'] = bollinger.bollinger_lband()
            df['BB_mid'] = bollinger.bollinger_mavg()
            df['BB_width'] = df['BB_high'] - df['BB_low']
            df['BB_position'] = (df['Close'] - df['BB_low']) / df['BB_width']
            
            # RSI
            df['RSI'] = ta.momentum.rsi(df['Close'])
            
            # Stochastic Oscillator
            df['Stoch_K'] = ta.momentum.stoch(df['High'], df['Low'], df['Close'])
            df['Stoch_D'] = ta.momentum.stoch_signal(df['High'], df['Low'], df['Close'])
            
            # Volume indicators
            df['Volume_SMA'] = ta.volume.volume_sma(df['Close'], df['Volume'])
            df['OBV'] = ta.volume.on_balance_volume(df['Close'], df['Volume'])
            
            # Volatility indicators
            df['ATR'] = ta.volatility.average_true_range(df['High'], df['Low'], df['Close'])
            
            # Price-based features
            df['Price_Change'] = df['Close'].pct_change()
            df['High_Low_Pct'] = (df['High'] - df['Low']) / df['Close']
            df['Open_Close_Pct'] = (df['Close'] - df['Open']) / df['Open']
            df['Volume_Change'] = df['Volume'].pct_change()
            
            # Rolling statistics
            df['Price_Volatility'] = df['Price_Change'].rolling(window=20).std()
            df['Price_Momentum'] = df['Close'] / df['Close'].shift(10) - 1
            
            # Support and Resistance
            df['Support'] = df['Low'].rolling(window=20).min()
            df['Resistance'] = df['High'].rolling(window=20).max()
            df['SR_Ratio'] = (df['Close'] - df['Support']) / (df['Resistance'] - df['Support'])
            
            # Day of week and month effects
            df['DayOfWeek'] = df.index.dayofweek
            df['Month'] = df.index.month
            df['DayOfMonth'] = df.index.day
            
            # Clean data
            df = df.replace([np.inf, -np.inf], np.nan)
            df = df.fillna(method='ffill').fillna(method='bfill')
            df = df.dropna()
            
            print(f"‚úÖ Calculated {len(df.columns)} features for {len(df)} data points")
            return df
            
        except Exception as e:
            print(f"‚ùå Error calculating indicators: {e}")
            return None
    
    def prepare_training_data(self, data, target_col='Close'):
        """
        Prepare data for training with proper feature selection
        """
        print("üéØ Preparing training data...")
        
        # Select features (exclude non-numeric and target)
        exclude_cols = ['Symbol', target_col]
        feature_cols = [col for col in data.columns if col not in exclude_cols]
        
        # Ensure all features are numeric
        numeric_features = []
        for col in feature_cols:
            if data[col].dtype in ['float64', 'int64']:
                numeric_features.append(col)
        
        self.feature_columns = numeric_features
        print(f"‚úÖ Selected {len(numeric_features)} numeric features")
        
        # Prepare arrays
        features = data[numeric_features].values
        targets = data[target_col].values
        
        # Scale features and targets
        features_scaled = self.feature_scaler.fit_transform(features)
        targets_scaled = self.scaler.fit_transform(targets.reshape(-1, 1)).flatten()
        
        # Create sequences
        X, y = [], []
        for i in range(self.lookback_period, len(features_scaled)):
            X.append(features_scaled[i-self.lookback_period:i])
            
            # Multi-step prediction
            if i + self.prediction_horizon <= len(targets_scaled):
                y.append(targets_scaled[i:i+self.prediction_horizon])
            else:
                y.append(targets_scaled[i:])  # Use remaining data
        
        X = np.array(X)
        y = np.array(y)
        
        print(f"‚úÖ Created sequences: X={X.shape}, y={y.shape}")
        return X, y
    
    def build_production_model(self, input_shape, output_shape):
        """
        Build a production-ready model with better architecture
        """
        print("üèóÔ∏è Building production model...")
        
        model = Sequential([
            # Input layer
            Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape, padding='same'),
            Dropout(0.2),
            
            # Feature extraction
            Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),
            Dropout(0.2),
            MaxPooling1D(pool_size=2),
            
            # Temporal processing
            LSTM(100, return_sequences=True, dropout=0.2, recurrent_dropout=0.2),
            LSTM(50, return_sequences=False, dropout=0.2, recurrent_dropout=0.2),
            
            # Dense layers
            Dense(50, activation='relu'),
            Dropout(0.3),
            Dense(25, activation='relu'),
            Dropout(0.2),
            
            # Output layer
            Dense(output_shape, activation='linear')
        ])
        
        # Compile with adaptive learning rate
        optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)
        model.compile(optimizer=optimizer, loss='mse', metrics=['mae', 'mape'])
        
        print(f"‚úÖ Model built with {model.count_params()} parameters")
        return model
    
    def train_model(self, X_train, y_train, X_val, y_val, epochs=100, batch_size=32):
        """
        Train model with comprehensive callbacks and monitoring
        """
        print("üéì Training model...")
        
        # Build model
        self.model = self.build_production_model(
            input_shape=(X_train.shape[1], X_train.shape[2]),
            output_shape=y_train.shape[1] if len(y_train.shape) > 1 else 1
        )
        
        # Callbacks for better training
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=20,
                restore_best_weights=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=10,
                min_lr=0.0001,
                verbose=1
            )
        ]
        
        # Train model
        self.history = self.model.fit(
            X_train, y_train,
            validation_data=(X_val, y_val),
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=1,
            shuffle=True
        )
        
        # Save model and scalers
        model_path = self.model_dir / f"nifty_model_{datetime.now().strftime('%Y%m%d_%H%M')}.h5"
        self.model.save(str(model_path))
        
        # Save scalers
        with open(self.model_dir / 'scalers.pkl', 'wb') as f:
            pickle.dump({
                'feature_scaler': self.feature_scaler,
                'target_scaler': self.scaler,
                'feature_columns': self.feature_columns
            }, f)
        
        print(f"‚úÖ Model saved to {model_path}")
        return self.history
    
    def make_predictions(self, X_test):
        """
        Make predictions with confidence intervals
        """
        if self.model is None:
            print("‚ùå Model not trained yet!")
            return None
        
        print("üîÆ Making predictions...")
        
        # Make predictions
        predictions_scaled = self.model.predict(X_test, verbose=0)
        
        # Inverse transform
        if len(predictions_scaled.shape) == 1:
            predictions_scaled = predictions_scaled.reshape(-1, 1)
        
        predictions = []
        for pred in predictions_scaled:
            pred_unscaled = self.scaler.inverse_transform(pred.reshape(-1, 1))
            predictions.append(pred_unscaled.flatten())
        
        predictions = np.array(predictions)
        print(f"‚úÖ Generated {len(predictions)} predictions")
        
        return predictions
    
    def evaluate_model(self, X_test, y_test):
        """
        Comprehensive model evaluation with multiple metrics
        """
        print("üìä Evaluating model performance...")
        
        predictions = self.make_predictions(X_test)
        if predictions is None:
            return None
        
        # Inverse transform actual values
        y_test_unscaled = []
        for y in y_test:
            y_unscaled = self.scaler.inverse_transform(y.reshape(-1, 1))
            y_test_unscaled.append(y_unscaled.flatten())
        y_test_unscaled = np.array(y_test_unscaled)
        
        # Calculate metrics for each prediction horizon
        metrics = {}
        for i in range(predictions.shape[1]):
            pred_i = predictions[:, i]
            actual_i = y_test_unscaled[:, i] if y_test_unscaled.shape[1] > i else y_test_unscaled[:, -1]
            
            mse = mean_squared_error(actual_i, pred_i)
            mae = mean_absolute_error(actual_i, pred_i)
            rmse = np.sqrt(mse)
            
            # Directional accuracy
            if len(actual_i) > 1:
                actual_direction = np.diff(actual_i) > 0
                pred_direction = np.diff(pred_i) > 0
                directional_accuracy = np.mean(actual_direction == pred_direction) * 100
            else:
                directional_accuracy = 0
            
            metrics[f'Day_{i+1}'] = {
                'MSE': mse,
                'MAE': mae,
                'RMSE': rmse,
                'Directional_Accuracy': directional_accuracy
            }
        
        return metrics, predictions, y_test_unscaled
    
    def generate_live_signals(self, latest_data, current_price):
        """
        Generate live trading signals based on latest predictions
        """
        print("üì° Generating live trading signals...")
        
        if len(latest_data) < self.lookback_period:
            print("‚ùå Insufficient data for prediction")
            return None
        
        # Prepare latest sequence
        features = latest_data[self.feature_columns].values
        features_scaled = self.feature_scaler.transform(features)
        
        # Get last sequence
        last_sequence = features_scaled[-self.lookback_period:].reshape(1, self.lookback_period, -1)
        
        # Make prediction
        prediction_scaled = self.model.predict(last_sequence, verbose=0)
        prediction = self.scaler.inverse_transform(prediction_scaled.reshape(-1, 1)).flatten()
        
        # Generate signals
        signals = []
        confidence_levels = []
        
        for i, pred_price in enumerate(prediction):
            price_change_pct = ((pred_price - current_price) / current_price) * 100
            
            # Signal logic with confidence
            if price_change_pct > 1.5:
                signal = "STRONG_BUY"
                confidence = min(abs(price_change_pct) * 10, 100)
            elif price_change_pct > 0.5:
                signal = "BUY"
                confidence = min(abs(price_change_pct) * 15, 100)
            elif price_change_pct < -1.5:
                signal = "STRONG_SELL"
                confidence = min(abs(price_change_pct) * 10, 100)
            elif price_change_pct < -0.5:
                signal = "SELL"
                confidence = min(abs(price_change_pct) * 15, 100)
            else:
                signal = "HOLD"
                confidence = max(50 - abs(price_change_pct) * 20, 0)
            
            signals.append(signal)
            confidence_levels.append(confidence)
        
        return {
            'predictions': prediction,
            'signals': signals,
            'confidence': confidence_levels,
            'current_price': current_price,
            'timestamp': datetime.now()
        }
    
    def plot_comprehensive_results(self, predictions, actual, metrics):
        """
        Create comprehensive visualization of results
        """
        print("üìà Creating visualization...")
        
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(20, 12))
        fig.suptitle('Nifty Prediction Model - Comprehensive Results', fontsize=16, fontweight='bold')
        
        # Plot 1: Predictions vs Actual
        axes[0, 0].plot(actual[-100:, 0], label='Actual', color='blue', linewidth=2, alpha=0.8)
        axes[0, 0].plot(predictions[-100:, 0], label='Predicted', color='red', linewidth=2, alpha=0.8)
        axes[0, 0].set_title('Predictions vs Actual (Last 100 points)', fontweight='bold')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_ylabel('Price')
        
        # Plot 2: Residuals
        residuals = actual[-100:, 0] - predictions[-100:, 0]
        axes[0, 1].plot(residuals, color='green', alpha=0.7)
        axes[0, 1].axhline(y=0, color='black', linestyle='--', alpha=0.5)
        axes[0, 1].set_title('Prediction Residuals', fontweight='bold')
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].set_ylabel('Residual')
        
        # Plot 3: Training History
        if self.history:
            axes[1, 0].plot(self.history.history['loss'], label='Training Loss', color='blue')
            axes[1, 0].plot(self.history.history['val_loss'], label='Validation Loss', color='orange')
            axes[1, 0].set_title('Training History', fontweight='bold')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)
            axes[1, 0].set_ylabel('Loss')
            axes[1, 0].set_xlabel('Epoch')
        
        # Plot 4: Metrics Comparison
        if metrics:
            metric_names = []
            directional_acc = []
            
            for day, day_metrics in metrics.items():
                metric_names.append(day)
                directional_acc.append(day_metrics['Directional_Accuracy'])
            
            bars = axes[1, 1].bar(metric_names, directional_acc, color='skyblue', alpha=0.8)
            axes[1, 1].set_title('Directional Accuracy by Prediction Day', fontweight='bold')
            axes[1, 1].set_ylabel('Accuracy (%)')
            axes[1, 1].set_ylim(0, 100)
            
            # Add value labels on bars
            for bar, acc in zip(bars, directional_acc):
                axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                               f'{acc:.1f}%', ha='center', fontweight='bold')
        
        plt.tight_layout()
        plt.show()
        
        # Save plot
        plot_path = self.data_dir / f"results_{datetime.now().strftime('%Y%m%d_%H%M')}.png"
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        print(f"‚úÖ Plot saved to {plot_path}")
    
    def run_live_prediction_pipeline(self, retrain=True):
        """
        Complete live prediction pipeline
        """
        print("üöÄ Starting Live Nifty Prediction Pipeline")
        print("=" * 80)
        
        # Step 1: Fetch live data
        raw_data = self.fetch_live_nifty_data()
        if raw_data is None or len(raw_data) < 100:
            print("‚ùå Failed to fetch sufficient data")
            return None
        
        # Step 2: Calculate indicators
        processed_data = self.calculate_advanced_indicators(raw_data)
        if processed_data is None:
            print("‚ùå Failed to calculate indicators")
            return None
        
        # Step 3: Prepare data
        X, y = self.prepare_training_data(processed_data)
        if len(X) < 100:
            print("‚ùå Insufficient processed data")
            return None
        
        # Step 4: Split data
        split_idx = int(len(X) * 0.8)
        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        
        X_train, X_val, y_train, y_val = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42, shuffle=False
        )
        
        print(f"‚úÖ Data split - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}")
        
        # Step 5: Train or load model
        if retrain or not (self.model_dir / 'scalers.pkl').exists():
            self.train_model(X_train, y_train, X_val, y_val, epochs=50)
        else:
            print("üìÇ Loading existing model...")
            # Load latest model
            model_files = list(self.model_dir.glob("nifty_model_*.h5"))
            if model_files:
                latest_model = max(model_files, key=os.path.getctime)
                self.model = load_model(str(latest_model))
                
                # Load scalers
                with open(self.model_dir / 'scalers.pkl', 'rb') as f:
                    scaler_data = pickle.load(f)
                    self.feature_scaler = scaler_data['feature_scaler']
                    self.scaler = scaler_data['target_scaler']
                    self.feature_columns = scaler_data['feature_columns']
                
                print(f"‚úÖ Loaded model from {latest_model}")
        
        # Step 6: Evaluate
        metrics, predictions, actual = self.evaluate_model(X_test, y_test)
        
        # Step 7: Print results
        print("\nüìä Model Performance:")
        print("-" * 50)
        for day, day_metrics in metrics.items():
            print(f"{day}:")
            for metric, value in day_metrics.items():
                if 'Accuracy' in metric:
                    print(f"  {metric}: {value:.2f}%")
                else:
                    print(f"  {metric}: {value:.4f}")
            print()
        
        # Step 8: Generate live signals
        current_price = processed_data['Close'].iloc[-1]
        live_signals = self.generate_live_signals(processed_data, current_price)
        
        print("üì° LIVE TRADING SIGNALS:")
        print("-" * 50)
        print(f"Current Price: ‚Çπ{current_price:.2f}")
        print(f"Timestamp: {live_signals['timestamp']}")
        print()
        
        for i, (pred, signal, conf) in enumerate(zip(
            live_signals['predictions'], 
            live_signals['signals'], 
            live_signals['confidence']
        )):
            change_pct = ((pred - current_price) / current_price) * 100
            print(f"Day {i+1}: {signal} | Target: ‚Çπ{pred:.2f} | Change: {change_pct:+.2f}% | Confidence: {conf:.0f}%")
        
        # Step 9: Visualize
        self.plot_comprehensive_results(predictions, actual, metrics)
        
        # Step 10: Summary
        avg_accuracy = np.mean([m['Directional_Accuracy'] for m in metrics.values()])
        
        print(f"\nüéâ Pipeline completed successfully!")
        print(f"üìä Average Directional Accuracy: {avg_accuracy:.2f}%")
        print(f"üíæ All results saved in: {self.data_dir}")
        
        return {
            'model': self.model,
            'metrics': metrics,
            'predictions': predictions,
            'actual': actual,
            'live_signals': live_signals,
            'processed_data': processed_data
        }

def main():
    """
    Main execution function
    """
    print("üåü Welcome to Live Nifty Prediction System!")
    print("This system will:")
    print("1. Fetch live Nifty data")
    print("2. Calculate technical indicators") 
    print("3. Train/load AI model")
    print("4. Generate predictions")
    print("5. Provide trading signals")
    print()
    
    # Create predictor
    predictor = LiveNiftyPredictor(lookback_period=60, prediction_horizon=3)
    
    # Run pipeline
    results = predictor.run_live_prediction_pipeline(retrain=True)
    
    if results:
        print("\n" + "="*80)
        print("üö® IMPORTANT DISCLAIMERS:")
        print("‚Ä¢ This is for educational and research purposes only")
        print("‚Ä¢ Past performance does not guarantee future results")
        print("‚Ä¢ Always do your own research before trading")
        print("‚Ä¢ Consider risk management and position sizing")
        print("‚Ä¢ Markets can be unpredictable due to external factors")
        print("="*80)
        
        return results
    else:
        print("‚ùå Pipeline failed. Please check the errors above.")
        return None

if __name__ == "__main__":
    # Set up proper display for matplotlib in VS Code
    plt.ion()  # Turn on interactive mode
    
    # Run the main pipeline
    results = main()
    
    # Keep the plots open
    plt.show(block=True)